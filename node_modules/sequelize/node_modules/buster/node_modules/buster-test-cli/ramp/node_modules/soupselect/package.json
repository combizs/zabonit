{
  "name": "soupselect",
  "version": "0.2.0",
  "engines": {
    "node": ">=0.2.0"
  },
  "author": {
    "name": "Harry Fuecks",
    "email": "hfuecks@gmail.com",
    "url": "http://twitter.com/hfuecks"
  },
  "url": "http://github.com/harryf/node-soupselect",
  "dependencies": {
    "htmlparser": ">= 1.6.2",
    "nodeunit": ">= 0.3.0"
  },
  "repository": [
    {
      "type": "git",
      "url": "git://github.com/harryf/node-soupselect.git"
    }
  ],
  "main": "./lib/soupselect",
  "license": "MIT",
  "description": "Adds CSS selector support to htmlparser for scraping activities - port of soupselect (python)",
  "contributors": [
    {
      "name": "Simon Willison https://github.com/simonw"
    },
    {
      "name": "Harry Fuecks https://github.com/harryf"
    },
    {
      "name": "Chris O'Hara https://github.com/chriso"
    }
  ],
  "readme": "node-soupselect\n---------------\n\nA port of Simon Willison's [soupselect](http://code.google.com/p/soupselect/) for use with node.js and node-htmlparser.\n\n    $ npm install soupselect\n\nMinimal example...\n\n    var select = require('soupselect').select;\n    // dom provided by htmlparser...\n    select(dom, \"#main a.article\").forEach(function(element) {//...});\n\nWanted a friendly way to scrape HTML using node.js. Tried using [jsdom](http://github.com/tmpvar/jsdom), prompted by [this article](http://blog.nodejitsu.com/jsdom-jquery-in-5-lines-on-nodejs) but, unfortunately, [jsdom](http://github.com/tmpvar/jsdom) takes a strict view of lax HTML making it unusable for scraping the kind of soup found in real world web pages. Luckily [htmlparser](http://github.com/tautologistics/node-htmlparser/) is more forgiving. More details on this found [here](http://www.reddit.com/r/node/comments/dm0tz/nodesoupselect_for_scraping_html_with_css/c118r23).\n\nA complete example including fetching HTML etc...;\n\n    var select = require('soupselect').select,\n        htmlparser = require(\"htmlparser\"),\n        http = require('http'),\n        sys = require('sys');\n\n    // fetch some HTML...\n    var http = require('http');\n    var host = 'www.reddit.com';\n    var client = http.createClient(80, host);\n    var request = client.request('GET', '/',{'host': host});\n\n    request.on('response', function (response) {\n        response.setEncoding('utf8');\n    \n        var body = \"\";\n        response.on('data', function (chunk) {\n            body = body + chunk;\n        });\n    \n        response.on('end', function() {\n        \n            // now we have the whole body, parse it and select the nodes we want...\n            var handler = new htmlparser.DefaultHandler(function(err, dom) {\n                if (err) {\n                    sys.debug(\"Error: \" + err);\n                } else {\n                \n                    // soupselect happening here...\n                    var titles = select(dom, 'a.title');\n                \n                    sys.puts(\"Top stories from reddit\");\n                    titles.forEach(function(title) {\n                        sys.puts(\"- \" + title.children[0].raw + \" [\" + title.attribs.href + \"]\\n\");\n                    })\n                }\n            });\n\n            var parser = new htmlparser.Parser(handler);\n            parser.parseComplete(body);\n        });\n    });\n    request.end();\n\nNotes:\n\n* Requires node-htmlparser > 1.6.2 & node.js 2+\n* Calls to select are synchronous - not worth trying to make it asynchronous IMO given the use case\n\n",
  "_id": "soupselect@0.2.0",
  "_from": "soupselect@~0.2"
}
